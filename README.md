# RLHF-пайплайн с REINFORCE ("Back to Basics")

Этот проект представляет собой реализацию полного RLHF (Reinforcement Learning from Human Feedback) пайплайна, основанного на алгоритме REINFORCE с baseline. Цель — продемонстрировать классический подход к дообучению языковых моделей.

## Быстрый старт

### 1. Установка окружения

Для развертывания проекта и установки зависимостей используйте `uv`:

```bash
# Создание и активация виртуального окружения
uv venv .venv
source .venv/bin/activate

# Установка зависимостей и проекта в режиме редактирования
uv sync --extra dev

# (Опционально) Установка pre-commit хуков для линтинга
pre-commit install
```

### 2. Запуск пайплайна

Весь процесс разделен на четыре последовательных шага, которые запускаются через `Makefile`:

```bash
# Шаг 1: Загрузка SFT (Supervised Fine-Tuning) модели
make sft

# Шаг 2: Обучение Reward Model (RM)
make rm

# Шаг 3: Обучение RLHF модели с помощью REINFORCE
make reinforce

# Шаг 4: Финальная валидация и сравнение моделей
make eval
```

## Структура проекта

-   `src/`: Основной исходный код, используемый в скриптах.
-   `scripts/`: Исполняемые скрипты для каждого шага пайплайна.
-   `models/`: Директория для сохранения весов обученных моделей (SFT, RM, RLHF).
-   `reports/`: Отчеты, метрики и анализ проведенных экспериментов.

## Результаты и выводы

Ключевой вывод проекта: для уже сильной SFT-модели базовый алгоритм REINFORCE не дает прироста в качестве, а его **неправильная реализация** приводит к значительной деградации.

**Все этапы работы, включая возникшие трудности, их решения и финальные результаты, подробно задокументированы в [`reports/README.md`](./reports/README.md).**
