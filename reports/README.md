# Отчёты

В этом документе отслеживаются проведенные эксперименты.

## Шаг 1: SFT модель

- **Модель**: `HuggingFaceTB/SmolLM2-135M-Instruct`
- **Действие**: Модель была успешно загружена с помощью скрипта `scripts/download_sft.py` и сохранена в `models/sft`. Этот этап прошел без затруднений.

## Шаг 2: Reward Model

Это был самый сложный и итеративный этап проекта.

- **Конфигурация**:
  - **Первоначальный подход**: Изначально для обучения использовался `trl.RewardTrainer`. Однако этот подход был оставлен из-за непреодолимых технических проблем, включая ошибки нехватки памяти (`CUDA OutOfMemoryError`), которые не решались ни градиентной аккумуляцией, ни чекпоинтингом, а также конфликты типов данных (`BFloat16` vs `fp16`) и внутренние ошибки `accelerate`.
  - **Финальный подход (ручной цикл)**: Из-за проблем с `Trainer` был написан собственный цикл обучения в `scripts/train_rm_manual.py`. В процессе его отладки была решена проблема `loss=nan` с помощью `torch.amp.GradScaler` и корректной настройки смешанной точности.
  - **Датасет**: `juyoungml/HelpSteer2-binarized`
  - **Гиперпараметры**: lr=5e-5, epochs=1, batch_size=1, fp16 (с `GradScaler`)
- **Метрики**:
  - **Validation Loss**: 0.6464
  - **Validation Accuracy**: 0.6032
- **Выводы**: Модель успешно обучилась, показав точность ~60% на валидационной выборке. Это лучше случайного угадывания и означает, что RM научилась различать предпочтительные ответы.

## Шаг 3: REINFORCE w/ baseline

Этот этап также потребовал глубокой отладки и исправления теоретических ошибок в реализации.

- **Первоначальная реализация**: Первые запуски показали крайне нестабильное обучение: огромные колебания `loss` и `reward`. Валидация показала, что RLHF-модель деградировала по сравнению с исходной SFT.
- **Анализ и исправления**: Глубокий анализ кода в сравнении с математикой из статьи "Back to Basics" выявил две критические ошибки:
    1.  **Неправильное обновление baseline**: Baseline обновлялся только "сырой" наградой от RM, а должен был полной KL-образной наградой (`R(x,y) = r_φ(x, y) - β * KL`).
    2.  **Неправильная область расчета градиентов**: `loss` вычислялся по всей последовательности (`prompt + response`), а должен был только по сгенерированному `response`.
- **Финальная реализация**: Скрипт `scripts/train_reinforce.py` был полностью переписан для исправления этих ошибок. Из вычисления награды был убран `torch.sigmoid` для работы с сырыми логитами RM. Была добавлена маска для токенов ответа, исправлена логика обновления baseline и скорректированы гиперпараметры.
- **Финальная конфигурация**:
    - **SFT-модель**: `models/sft`
    - **RM-модель**: `models/rm`
    - **Гиперпараметры**: lr=2e-6, batch_size=2, kl_beta=0.1, gradient_accumulation_steps=8

## Шаг 4: Валидация

- **Первоначальные результаты (до исправления REINFORCE)**:
    - **Mean SFT Reward**: `0.3480`
    - **Mean RLHF Reward**: `0.3462`
    - **Анализ**: Неправильная реализация REINFORCE приводила к деградации модели.

- **Финальные результаты (после исправления REINFORCE)**:
    - **Mean SFT Reward**: `0.6432`
    - **Mean RLHF Reward**: `0.6389`
    - **Анализ**: После исправления критических ошибок в реализации REINFORCE удалось добиться стабильного обучения. Финальный результат RLHF-модели практически идентичен SFT-модели. Это подтверждает гипотезу о том, что для уже сильной SFT-модели базовый алгоритм REINFORCE не дает дополнительного выигрыша, а его некорректная реализация приводит к деградации.
