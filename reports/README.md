# Отчёты

В этом документе отслеживаются проведенные эксперименты.

## Шаг 1: SFT модель

- **Модель**: `HuggingFaceTB/SmolLM2-135M-Instruct`
- **Действие**: Модель загружена и сохранена в `models/sft`.

## Шаг 2: Reward Model

- **Конфигурация**:
  - **Подход**: Изначально планировалось использовать `RewardTrainer` из `trl`. Однако, в процессе возникли многочисленные ошибки, связанные с нехваткой памяти (CUDA OOM) и конфликтами версий библиотек. Для обеспечения стабильности и полного контроля над процессом было принято решение реализовать ручной цикл обучения. Этот подход позволил обойти внутренние сложности `Trainer` и успешно обучить модель на доступном оборудовании.
  - **Датасет**: `juyoungml/HelpSteer2-binarized`
  - **Гиперпараметры**: lr=5e-5, epochs=1, batch_size=1, fp16 (с `GradScaler`)
- **Метрики**:
  - **Validation Loss**: 0.6464
  - **Validation Accuracy**: 0.6032
- **Выводы**: Модель успешно обучилась, показав точность ~60% на валидационной выборке. Это значительно лучше случайного угадывания (50%) и означает, что модель научилась различать предпочтительные и нежелательные ответы. Полученная Reward Model готова для использования на следующем шаге.

## Шаг 3: REINFORCE w/ baseline

- **Конфигурация**:
    - **SFT-модель**: `models/sft`
    - **RM-модель**: `models/rm`
    - **Гиперпараметры**: lr=1e-5, batch_size=2, kl_beta=0.2, baseline_alpha=0.9
- **Метрики (полный прогон, 1 эпоха)**:
    - **Средняя награда**: крайне нестабильна на протяжении всего обучения, без явного тренда к росту. Значения колеблются в широком диапазоне.
    - **KL-штраф**: также демонстрирует высокую вариативность, что говорит о периодических сильных отклонениях политики от исходной SFT-модели.
- **Выводы**: Полный цикл обучения завершен, модель сохранена. Главный вывод — классическая для REINFORCE высокая дисперсия градиентов делает процесс очень нестабильным.

## Шаг 4: Валидация

- **Результаты**:
    - **Mean SFT Reward**: `0.3480`
    - **Mean RLHF Reward**: `0.3462`
- **Анализ**: RLHF-модель, обученная с помощью REINFORCE, показала результат хуже, чем исходная SFT-модель. Разница минимальна и, вероятно, находится в пределах статистической погрешности, однако это однозначно говорит об отсутствии улучшения. Это подтверждает, что высокая дисперсия градиентов и нестабильность во время обучения не позволили модели найти лучшую политику. Для улучшения результатов, вероятно, стоило бы использовать более продвинутый алгоритм, например PPO, который является стандартом в современных RLHF-пайплайнах.
- **Финальные итоги (Корректная реализация REINFORCE)**:
    - **Конфигурация**: lr=2e-6, kl_beta=0.1, gradient_accumulation=8
    - **Результаты**:
        - **Mean SFT Reward**: 0.6432
        - **Mean RLHF Reward**: 0.6423
    - **Анализ**: После исправления критических ошибок в реализации (обновление baseline полной KL-образной наградой и корректный расчет градиентов только по токенам ответа) удалось добиться полностью стабильного обучения. Финальный результат RLHF-модели практически идентичен SFT-модели, что является успехом, так как мы остановили деградацию и подтвердили гипотезу о том, что для уже сильной SFT-модели базовый REINFORCE не дает дополнительного выигрыша. Это идеальный пример, показывающий важность точного соответствия кода математической формуле из первоисточника.
